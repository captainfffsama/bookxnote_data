{"EpubVersion":1,"filepath":"","floatingtheme":[],"folded":false,"markups":[{"date":"2021-08-18 18:12:14","docid":0,"fillcolor":"ffffed99","id":1,"linecolor":"ffff0000","originaltext":"The key principle be-hind these techniques is a feature matching between the testsample and the nominal samples while exploiting the multi-scale nature of deep feature representations. Subtle, fine-grained defect segmentation is covered by high-resolutionfeatures, whereas structural deviations and full image-levelanomaly detection are supposed to be covered by featuresat much higher abstraction levels.","page":0,"textblocks":[{"first":[455.59112548828125,582.242431640625,6.087158203125,12.00494384765625],"last":[438.8140563964844,665.929443359375,2.49066162109375,12.00494384765625],"length":412,"rects":[[455.59112548828125,582.242431640625,89.52398681640625,12.00494384765625],[308.86199951171875,594.1984252929688,236.25311279296875,12.00494384765625],[308.86199951171875,606.1534423828125,236.25299072265625,12.00494384765625],[308.86199951171875,618.1084594726562,236.2530517578125,12.00494384765625],[308.86199951171875,630.0634765625,236.2530517578125,12.00494384765625],[308.86199951171875,642.0184326171875,236.2530517578125,12.00494384765625],[308.86199951171875,653.9734497070312,236.2530517578125,12.00494384765625],[308.86199951171875,665.929443359375,132.44271850585938,12.00494384765625]],"start":3314,"text":"The key principle be-\nhind these techniques is a feature matching between the test\nsample and the nominal samples while exploiting the multi-\nscale nature of deep feature representations. Subtle, fine-\ngrained defect segmentation is covered by high-resolution\nfeatures, whereas structural deviations and full image-level\nanomaly detection are supposed to be covered by features\nat much higher abstraction levels."}],"type":5,"underline":true},{"date":"2021-08-18 18:16:56","docid":0,"fillcolor":"ffffed99","id":4,"linecolor":"ffff0000","originaltext":"(1) maximizing nominal information available attest time, (2) reducing biases towards ImageNet classesand (3) retaining high inference speeds.","page":1,"textblocks":[{"first":[83.10814666748047,132.5986328125,3.3175430297851562,12.004928588867188],"last":[213.7178497314453,156.5096435546875,2.4906463623046875,12.004928588867188],"length":144,"rects":[[83.10814666748047,132.5986328125,203.25690460205078,12.004928588867188],[50.11201477050781,144.55364990234375,236.2531280517578,12.004928588867188],[50.11201477050781,156.5096435546875,166.0964813232422,12.004928588867188]],"start":287,"text":"(1) maximizing nominal information available at\ntest time, (2) reducing biases towards ImageNet classes\nand (3) retaining high inference speeds."}],"type":5,"underline":true},{"CL":[46.896552842238876,140.68965852671664,27.133005573009637,143.83251474314727,27.133005573009637,123.83251474314727],"date":"2021-08-18 18:17:12","docid":0,"id":5,"linecolor":"ffd01a11","linewidth":4,"originaltext":"本文方法三个优势","page":1,"rect":[6.02955679400214,101.83251474314727,49.236454352017134,124.83251474314727],"type":9},{"CL":[376.8472996251338,715.1724308441429,365.45813679201865,737.9507565103731,365.45813679201865,744.6502640592645],"date":"2021-08-18 18:20:13","docid":0,"id":6,"linecolor":"ffd01a11","linewidth":3,"originaltext":"现存问题1：现有方法现成的特征提取器，在高层网络的语义和现在任务是不匹配的，导致高层语义可靠性下降。","page":0,"rect":[315.21183017533417,744.6502640592645,416.7044434087032,767.7586389706056],"type":9},{"date":"2021-08-18 18:21:06","docid":0,"fillcolor":"ffffed99","id":7,"linecolor":"ffff0000","originaltext":"n the specific case of ResNet-likearchitectures, this would refer to e.g. j ∈ [2, 3].","page":2,"textblocks":[{"first":[405.6585693359375,120.5787353515625,4.981292724609375,12.004928588867188],"last":[496.3449401855469,132.53375244140625,2.49066162109375,12.004928588867188],"length":86,"rects":[[405.6585693359375,120.5787353515625,139.45654296875,12.004928588867188],[308.8619384765625,132.53375244140625,189.97366333007812,12.004928588867188]],"start":3130,"text":"n the specific case of ResNet-like\narchitectures, this would refer to e.g. j ∈ [2, 3]."}],"type":5,"underline":true},{"CL":[548.0197174993058,128.63054493871235,546.1083878813122,136.33497861993732,566.1083878813122,136.33497861993732],"date":"2021-08-18 18:21:19","docid":0,"id":8,"linecolor":"ffd01a11","linewidth":3,"originaltext":"用了2，3block","page":2,"rect":[566.1083878813122,131.980298713158,603.6207040227696,141.68965852671664],"type":9},{"date":"2021-08-18 18:23:46","docid":0,"fillcolor":"ffffed99","id":9,"linecolor":"ffff0000","originaltext":"adap-tive average pooling.","page":2,"textblocks":[{"first":[522.9822998046875,538.376708984375,4.42340087890625,12.00494384765625],"last":[388.62237548828125,550.3317260742188,2.49066162109375,12.00494384765625],"length":27,"rects":[[522.9822998046875,538.376708984375,22.12701416015625,12.00494384765625],[308.8618469238281,550.3317260742188,82.25119018554688,12.00494384765625]],"start":4911,"text":"adap-\ntive average pooling."}],"type":5,"underline":true},{"date":"2021-08-18 18:24:16","docid":0,"id":10,"linecolor":"ffd01a11","linewidth":3,"originaltext":"所谓特征聚合方法，就是用自适应平均池化","page":2,"rect":[558.7389295775317,515.8620812646276,593.5714426994325,572.1330185429806],"type":11},{"date":"2021-08-18 18:33:50","docid":0,"fillcolor":"ffffed99","id":12,"linecolor":"ffff0000","originaltext":"we use a min-imax facility location coreset selection,","page":3,"textblocks":[{"first":[487.3018798828125,603.4662475585938,7.1929931640625,12.00494384765625],"last":[466.8233337402344,615.4222412109375,2.49066162109375,12.00494384765625],"length":55,"rects":[[487.3018798828125,603.4662475585938,57.810546875,12.00494384765625],[308.8619384765625,615.4222412109375,160.45205688476562,12.00494384765625]],"start":3111,"text":"we use a min-\nimax facility location coreset selection,"}],"type":5,"underline":true},{"date":"2021-08-18 18:34:37","docid":0,"fillcolor":"ffffed99","id":14,"linecolor":"ffff0000","originaltext":"To fur-ther reduce coreset selection time, we follow [49], makinguse of the Johnson-Lindenstrauss theorem [11] to reduce di-mensionalities of elements m ∈ M through random linearprojections ψ : Rd → Rd∗ with d∗ < d.","page":4,"textblocks":[{"first":[257.7923278808594,72.757568359375,6.087158203125,12.004936218261719],"last":[207.36102294921875,120.57763671875,2.4906463623046875,12.004928588867188],"length":219,"rects":[[257.7923278808594,72.757568359375,28.572723388671875,12.004936218261719],[50.11199951171875,84.71258544921875,236.25311279296875,12.004936218261719],[50.11199951171875,96.6676025390625,236.25308227539062,12.004936218261719],[50.11199951171875,108.62261962890625,236.25411987304688,12.004936218261719],[50.11199951171875,120.57763671875,159.7396697998047,12.004928588867188]],"start":50,"text":"To fur-\nther reduce coreset selection time, we follow [49], making\nuse of the Johnson-Lindenstrauss theorem [11] to reduce di-\nmensionalities of elements m ∈ M through random linear\nprojections ψ : Rd → Rd∗ with d∗ < d."}],"type":5,"underline":true},{"date":"2021-08-18 18:41:19","docid":0,"id":15,"linecolor":"ffd01a11","linewidth":3,"originaltext":"计算memory和待查的最近邻的结果中差异最大的，然后使用softmax进行重参数化","page":4,"rect":[61.97044482724422,733.2611012261493,160.78325504105675,749.6699685885992],"type":11},{"date":"2021-08-19 09:38:30","docid":0,"id":16,"imgfile":"9a5d29ced3c80e97e41997635631f085.png","linecolor":"ffa0ec6f","linewidth":2,"page":4,"rect":[47.90147897457257,467.6256269126105,299.80296281288423,713.49755395692],"type":2},{"CL":[47.90147897457257,584.8670090182077,50.246306616684514,601.9507532678805,50.246306616684514,608.6502608167717],"date":"2021-08-19 09:38:54","docid":0,"id":17,"linecolor":"ffd01a11","linewidth":3,"originaltext":"异常分数算法","page":4,"rect":[0,608.6502608167717,33.15763623467809,618.3596206303304],"type":9}],"maxid":17,"title":"Towards_Total_Recall_in_Industrial_Anomaly_Detection","unimportant":[]}